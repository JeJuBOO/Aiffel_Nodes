{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPputde+r8YpPtnCGf2OonH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 20. 정보이론\n","\n","--------------------------\n","### 목차\n","2. Information Content\n","  + 정보량(Information Content)의 개념에 대해 알아봅니다.\n","3. Entropy\n","  + 확률에서의 Entropy 개념에 대해 살펴봅니다.\n","4. Kullback Leibler Divergence\n","  + Kullback Leibler Divergence과 Cross Entropy의 관계에 대해 알아봅니다.\n","5. Cross Entropy Loss\n","  + Cross Entropy Loss에 대해 알아봅니다.\n","6. Decision Tree와 Entropy\n","  + 실습과 함께 Decision Tree와 Entropy에 대해 살펴봅니다.\n","\n","### 학습 목표\n","머신러닝 이론의 이론적 토대를 이루는 가장 중요한 이론 중 하나인 정보이론(Information Theory)에 대해 알아봅니다. 또한 머신러닝에서 많이 사용되는 Entropy, Cross Entropy, KL divergence 등의 개념과 이런 개념들이 머신러닝 이론과 연결되는 사례를 파악해 봅니다.\n","\n","---------------------------\n"],"metadata":{"id":"HdYV0BqKlSaB"}},{"cell_type":"markdown","source":["## 20-2. Information Content\n","\n","정보를 정량젹으로 표현하느 이론이다.\n","\n","정보를 정량적으로 표현하기 위해 필요한 세 가지 조건이 설명되어 있다.\n","  1. 일어날 가능성이 높은 사건은 정보량이 낮고, 반드시 일어나는 사건에는 정보가 없는 것이나 마찬가지입니다.\n","  2. 일어날 가능성이 낮은 사건은 정보량이 높습니다.\n","  3. 두 개의 독립적인 사건이 있을 때, 전체 정보량은 각각의 정보량을 더한 것과 같습니다.\n","\n","사건$x$가 일어날 확률이 $P(X=x)$라고 할 때, 사건의 **정보량(information content)**$I(x)$는 다음과 같이 정의된다.\n","$$I(x)=-log_bP(x)$$\n","여기서 $b$는 주로 2,e,10과 같은 값이 사용된다.\n"],"metadata":{"id":"S-ZmhEl3losg"}},{"cell_type":"markdown","source":["## 20-3. Entropy\n","\n","특정 확률분포를 따르는 사건들의 정보량 기댓값을 **엔트로피(entropy)**라고 한다.\n","\n","### For Discrete Random Variables\n","\n","이산 확률 변수 $X$가 $x_1,x_2,⋯,x_n$중 하나의 값을 가진다고 가정한다.\n","**엔트로피**는 각각의 경우의 수가 가지는 정보량에 확률을 곱한 후, 그 값을 모두 더한 값이다.\n","$$H(X)=\\mathbb{E}_{X\\sim P}[I(x)]=-\\sum^{n}_{i=1}p_i\\text{log}p_i\\quad (p_i:=P(X=x_i))$$\n","\n","![entropy](https://d3s0tskafalll9.cloudfront.net/media/images/math08_2-1.max-800x600.png)  "],"metadata":{"id":"s5R9CG-5ngLb"}},{"cell_type":"markdown","source":["### For Continuous Random Variables\n","\n","$X$가 연속 확률 변수일 때는 적분의 형태로 정의한다. 확률 변수$X$의 확률 밀도 함수가 $p(x)$일 때 엔트로피는 다음과 같다.\n","$$h(X)=-\\int p(x)\\text{log}p(x)dx$$\n","연속 확률 변수의 엔트로피를 이산 확률 변수와 구분하여 **미분 엔트로피(differential entropy)**라고 부르기도 한다."],"metadata":{"id":"IedgRX_c7IBf"}},{"cell_type":"markdown","source":["## Kullback Leibler Divergence\n","\n","머신러닝은 새로운 입력 데이터를 잘 예측 해야한다. 즉 모델의 확률 분포를 실제 확률 분포에 가깝게 만드는 것.\n","머신러닝 모델은 크게 두 가지가 있다.\n","우선 결정 모델(discriminative model)은 데이터의 결정 경계(decision boundary)를 학습한다. 그리고, 생성 모델(generative model)은 데이터와 모델로 도출할 수 있는 여러 확률 분포와 베이즈 이론을 이용해 데이터의 실제 분포를 간접적으로 모델링한다.  \n","\n","여기서 **쿨백-라이블러 발산(Kullback-Leibler divergence, KL divergence)**으 두 확률 분포의 차이를 나타내는 지표를 얻을 수 있다.  \n","데이터가 따르는 실제 확률 분포를 $P(X)$,모델이 나타내는 확률 분포를 $Q(X)$라고 할때.\n","$$D_{KL}(P||Q)=\\mathbb{E}_{X\\sim P}[-\\text{log}Q(x)]-\\mathbb{E}_{X\\sim P}[-\\text{log}P(x)]=\\sum P(x)log\\left( \\frac{P(x)}{Q(x)}\\right)$$\n","\n","KL divergence는 거리 함수와 비슷한 성질을 가지고 있지만 거리를 나타내는 것은 아니다. 이는 성질에 나타난다.  \n","* $D_{KL}(P||Q)\\geq0$\n","* $D_{KL}(P||Q)=0\\text{ if and only if }P=Q$\n","* $\\text{non-symmeric : } D_{KL}(P||Q)\\neq D_{KL}(Q||P)$\n","\n","위 $D_{KL}$을 최소화 하기 위해서 우리는 바꿀수 있는 부분을 최소화 해야한다. 이 식은 $P(x)$를 기준으로 계산한 $Q(x)$의 엔트로피, 즉 $P(x)$에 대한 $Q(x)$의 **교차 엔트로피(cross entropy)**이다.  \n","\n","### Cross Entropy\n","$P(x)$에 대한 $Q(x)$의 교차 엔트로피식을 다시 적으면 다음과 같다.\n","$$H(P,Q)=-\\mathbb{E}_{X\\sim P}[\\text{log}Q(x)]=-\\sum P(x)\\text{log}Q(x)$$\n","\n","KL divergence의 계산식으로부터 엔트로피와 교차 엔트로피, KL divergence 사이의 관계식을 얻을 수 있습니다.   \n","\n","![KLd_관계식](https://d3s0tskafalll9.cloudfront.net/media/images/math08_3-2.max-800x600.png)   \n","$$H(P,Q)=H(P)+D_{KL}(P||Q)$$\n"],"metadata":{"id":"VzDS-_pw8oXM"}},{"cell_type":"markdown","source":["## 20-5. Cross Entropy Loss\n","\n"],"metadata":{"id":"N5k5EEU1C3NV"}}]}